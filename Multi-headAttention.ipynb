{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_head = 2\n",
    "head_dim = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_q = nn.Linear(4, 4)\n",
    "linear_k = nn.Linear(4, 4)\n",
    "linear_v = nn.Linear(4, 4)\n",
    "\n",
    "linear_out = nn.Linear(4, 4)\n",
    "\n",
    "Q = linear_q(x)\n",
    "K = linear_k(x)\n",
    "V = linear_v(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分割\n",
    "def split_head(x, num_head):\n",
    "    batch_size, seq_len, feature_dim = x.size()\n",
    "    head_dim = feature_dim // num_head\n",
    "    output = x.view(batch_size, seq_len, num_head, head_dim).transpose(1, 2)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = split_head(Q, num_head)\n",
    "K = split_head(K, num_head)\n",
    "V = split_head(V, num_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity_score:\n",
      " tensor([[[[-1.3846, -0.3331,  0.3355],\n",
      "          [-0.5084, -0.0064,  0.2133],\n",
      "          [ 0.4380,  0.2477,  0.0046]],\n",
      "\n",
      "         [[ 0.7266,  0.2095, -0.7258],\n",
      "          [ 0.1571,  0.0338, -0.2635],\n",
      "          [-0.7530, -0.2057,  0.8573]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "size:\n",
      " torch.Size([1, 2, 3, 3])\n",
      "scale_factor:\n",
      " 1.7320508075688772\n",
      "scale_weight:\n",
      " tensor([[[[-0.7994, -0.1923,  0.1937],\n",
      "          [-0.2935, -0.0037,  0.1232],\n",
      "          [ 0.2529,  0.1430,  0.0026]],\n",
      "\n",
      "         [[ 0.4195,  0.1209, -0.4190],\n",
      "          [ 0.0907,  0.0195, -0.1522],\n",
      "          [-0.4347, -0.1188,  0.4949]]]], grad_fn=<DivBackward0>)\n",
      "size:\n",
      " torch.Size([1, 2, 3, 3])\n",
      "attn_weight:\n",
      " tensor([[[[0.1807, 0.3316, 0.4878],\n",
      "          [0.2595, 0.3468, 0.3937],\n",
      "          [0.3739, 0.3350, 0.2911]],\n",
      "\n",
      "         [[0.4599, 0.3412, 0.1988],\n",
      "          [0.3682, 0.3429, 0.2888],\n",
      "          [0.2039, 0.2796, 0.5165]]]], grad_fn=<SoftmaxBackward0>)\n",
      "size:\n",
      " torch.Size([1, 2, 3, 3])\n",
      "attn_output:\n",
      " tensor([[[[-2.0152,  0.6107],\n",
      "          [-0.7284,  0.1858],\n",
      "          [ 0.6517, -0.2403]],\n",
      "\n",
      "         [[ 0.6592, -0.9604],\n",
      "          [ 0.1866, -0.2652],\n",
      "          [-0.7266,  1.0520]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "size:\n",
      " torch.Size([1, 2, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "similarity_score = torch.matmul(Q, K.transpose(-2, -1))\n",
    "print('similarity_score:\\n', similarity_score)\n",
    "print('size:\\n', similarity_score.size())\n",
    "scale_factor = similarity_score.size(-1) ** 0.5\n",
    "print('scale_factor:\\n', scale_factor)\n",
    "scale_weight = similarity_score / scale_factor\n",
    "print('scale_weight:\\n', scale_weight)\n",
    "print('size:\\n', scale_weight.size())\n",
    "attn_weight = F.softmax(scale_weight, dim=-1)\n",
    "print('attn_weight:\\n', attn_weight)\n",
    "print('size:\\n', attn_weight.size())\n",
    "attn_output = torch.matmul(similarity_score, V)\n",
    "print('attn_output:\\n', attn_output)\n",
    "print('size:\\n', attn_output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并\n",
    "def combine_head(x):\n",
    "    batch_size, num_head, seq_len, head_dim = x.size()\n",
    "    feature_dim = num_head * head_dim\n",
    "    output = x.transpose(1, 2).contiguous().view(batch_size, seq_len, feature_dim)\n",
    "    # tensor.contiguous()确保张量内存布局是连续的，将非连续张量转为连续张量，如果张量已是连续，那么直接返回原张量\n",
    "    # 配合使用\n",
    "    # view()改变张量形状，要求张量内存布局必须连续\n",
    "    # 如在调用前对张量进行transpose()，permute()或切片等操作，可能会导致张量内存布局非连续，此时需先调用contiguous()\n",
    "    # 在GPU上，非连续张量可能会导致某些操作失败或效率低下，因此在将张量传递到GPU前，需确保其连续性\n",
    "    # 调用contiguous()会创建新张量\n",
    "    # 在某些情况下，contiguous()会引入额外的内存复制开销，建议只在必要时使用\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_output:\n",
      " tensor([[[-0.4509,  0.9675, -1.0888,  0.9564],\n",
      "         [-0.2544,  0.5803, -0.5386,  0.6358],\n",
      "         [-0.0670,  0.1700,  0.4976,  0.5800]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_output = linear_out(combine_head(attn_output))\n",
    "print('attn_output:\\n', attn_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
