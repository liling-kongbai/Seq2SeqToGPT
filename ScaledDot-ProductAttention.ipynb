{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化向量\n",
    "x1 = torch.randn(1, 3, 4)\n",
    "x2 = torch.randn(1, 5, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity_score:\n",
      " tensor([[[-0.0093, -0.4817, -1.2433, -0.3959, -0.6269],\n",
      "         [ 0.1143,  2.2590,  4.2237,  2.6393,  0.2414],\n",
      "         [-1.0587,  2.0005,  4.5182,  3.0956,  1.4810]]])\n"
     ]
    }
   ],
   "source": [
    "# 计算相似度得分\n",
    "similarity_score = torch.matmul(x1, x2.transpose(1, 2))\n",
    "print('similarity_score:\\n', similarity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 缩放\n",
    "缩放因子 = x1.size(-1) ** 0.5\n",
    "缩放权重 = similarity_score / 缩放因子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_weight:\n",
      " tensor([[[0.2572, 0.2031, 0.1388, 0.2120, 0.1889],\n",
      "         [0.0612, 0.1790, 0.4780, 0.2165, 0.0653],\n",
      "         [0.0299, 0.1382, 0.4865, 0.2389, 0.1066]]])\n"
     ]
    }
   ],
   "source": [
    "# 注意力权重\n",
    "attn_weight = F.softmax(缩放权重, dim=2)\n",
    "print('attn_weight:\\n', attn_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_output:\n",
      " tensor([[[ 0.2706,  0.7339,  0.2997, -0.1098],\n",
      "         [ 0.2968,  1.2934,  0.3733, -0.3443],\n",
      "         [ 0.2677,  1.2730,  0.4297, -0.3645]]])\n"
     ]
    }
   ],
   "source": [
    "# 计算注意力输出\n",
    "attn_output = torch.matmul(attn_weight, x2)\n",
    "print('attn_output:\\n', attn_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
