{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from collections import Counter\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正弦函数位置编码表\n",
    "def get_sin_function_positional_coding_table(max_len_sequence, dim_embedding):\n",
    "    sin_function_positional_coding_table = torch.zeros(max_len_sequence, dim_embedding)\n",
    "    for pos_i in range(max_len_sequence):\n",
    "        for hid_j in range(dim_embedding):\n",
    "            sin_function_positional_coding_table[pos_i, hid_j] = pos_i / torch.pow(torch.tensor(10000.0), 2 * (hid_j // 2) / dim_embedding)\n",
    "    sin_function_positional_coding_table[:, 0::2] = torch.sin(sin_function_positional_coding_table[:, 0::2])\n",
    "    sin_function_positional_coding_table[:, 1::2] = torch.sin(sin_function_positional_coding_table[:, 1::2])\n",
    "    return sin_function_positional_coding_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 填充注意力掩码\n",
    "def get_padding_attn_mask(sequence_Q, sequence_K):\n",
    "    batch_size, len_Q = sequence_Q.size()\n",
    "    batch_size, len_K = sequence_K.size()\n",
    "    padding_attn_mask = sequence_K.eq(0).unsqueeze(1).expand(batch_size, len_Q, len_K)\n",
    "    return padding_attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 缩放-点积-填充掩码-注意力\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, padding_attn_mask):\n",
    "        scale_weight = torch.matmul(Q, K.transpose(-2, -1)) / torch.pow(torch.tensor(Q.size(-1)), 2)\n",
    "        scale_weight.masked_fill(padding_attn_mask, -1e9)\n",
    "        attn_weight = F.softmax(scale_weight, dim=-1)\n",
    "        context = torch.matmul(attn_weight, V)\n",
    "        return context, attn_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多头-缩放-点积-填充掩码-注意力\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_embedding, dim_Q, dim_K, dim_V, n_head):\n",
    "        super().__init__()\n",
    "        self.dim_Q = dim_Q\n",
    "        self.dim_K = dim_K\n",
    "        self.dim_V = dim_V\n",
    "        self.n_head = n_head\n",
    "\n",
    "        self.multi_head_linear_Q = nn.Linear(dim_embedding, self.dim_Q * n_head)\n",
    "        self.multi_head_linear_K = nn.Linear(dim_embedding, self.dim_K * n_head)\n",
    "        self.multi_head_linear_V = nn.Linear(dim_embedding, self.dim_V * n_head)\n",
    "\n",
    "        self.MultiHeadAttention_ScaledDotProductAttention = ScaledDotProductAttention()\n",
    "        self.linear = nn.Linear(self.n_head * self.dim_V, dim_embedding)\n",
    "        self.layer_norm = nn.LayerNorm(dim_embedding)\n",
    "\n",
    "    def forward(self, Q, K, V, padding_attn_mask):\n",
    "        residual, batch_size = Q, Q.size(0)\n",
    "\n",
    "        multi_head_Q  = self.multi_head_linear_Q(Q).view(batch_size, -1, self.n_head, self.dim_Q).transpose(-3, -2)\n",
    "        multi_head_K  = self.multi_head_linear_K(K).view(batch_size, -1, self.n_head, self.dim_K).transpose(-3, -2)\n",
    "        multi_head_V  = self.multi_head_linear_V(V).view(batch_size, -1, self.n_head, self.dim_V).transpose(-3, -2)\n",
    "\n",
    "        multi_head_padding_attn_mask = padding_attn_mask.unsqueeze(1).repeat(1, self.n_head, 1, 1)\n",
    "\n",
    "        context, attn_weight = self.MultiHeadAttention_ScaledDotProductAttention(multi_head_Q, multi_head_K, multi_head_V, multi_head_padding_attn_mask)\n",
    "        context = context.transpose(-3, -2).contiguous().view(batch_size, -1, self.n_head * self.dim_V)\n",
    "        context = self.layer_norm(self.linear(context) + residual)\n",
    "        return context, attn_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 逐位-卷积-前馈网络\n",
    "class PositionWiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self, dim_embedding, dim_pwffn):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(dim_embedding, dim_pwffn, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(dim_pwffn, dim_embedding, kernel_size=1)\n",
    "        self.layer_norm = nn.LayerNorm(dim_embedding)\n",
    "\n",
    "    def forward(self, context):\n",
    "        residual = context\n",
    "        context = F.relu(self.conv1(context.transpose(1, 2)))\n",
    "        context = self.conv2(context).transpose(1, 2)\n",
    "        context = self.layer_norm(context + residual)\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编码器层\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, dim_embedding, dim_Q, dim_K, dim_V, n_head, dim_pwffn):\n",
    "        super().__init__()\n",
    "        self.EncoderLayer_MultiHeadAttention = MultiHeadAttention(dim_embedding, dim_Q, dim_K, dim_V, n_head)\n",
    "        self.EncoderLayer_PositionWiseFeedForwardNet = PositionWiseFeedForwardNet(dim_embedding, dim_pwffn)\n",
    "\n",
    "    def forward(self, encoder_layer_input, encoder_layer_padding_attn_mask):\n",
    "        encoder_layer_output, encoder_layer_output_attn_weight = self.EncoderLayer_MultiHeadAttention(encoder_layer_input, encoder_layer_input, encoder_layer_input, encoder_layer_padding_attn_mask)\n",
    "        encoder_layer_output = self.EncoderLayer_PositionWiseFeedForwardNet(encoder_layer_output)\n",
    "        return encoder_layer_output, encoder_layer_output_attn_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编码器\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, corpus, dim_embedding, dim_Q, dim_K, dim_V, n_head, dim_pwffn, n_layer):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(corpus.len_source_vocabulary, dim_embedding)\n",
    "        self.position_embedding = nn.Embedding.from_pretrained(get_sin_function_positional_coding_table(corpus.max_len_source_sequence + 1, dim_embedding), freeze=True)\n",
    "        self.layers = nn.ModuleList(EncoderLayer(dim_embedding, dim_Q, dim_K, dim_V, n_head, dim_pwffn) for _ in range(n_layer))\n",
    "\n",
    "    def forward(self, encoder_input):\n",
    "        positional_coding_template = torch.arange(1, encoder_input.size(1) + 1).unsqueeze(0)\n",
    "        encoder_output = self.token_embedding(encoder_input) + self.position_embedding(positional_coding_template)\n",
    "\n",
    "        encoder_padding_attn_mask = get_padding_attn_mask(encoder_input, encoder_input)\n",
    "\n",
    "        encoder_attn_weights = []\n",
    "        for layer in self.layers:\n",
    "            encoder_output, encoder_attn_weight = layer(encoder_output, encoder_padding_attn_mask)\n",
    "            encoder_attn_weights.append(encoder_attn_weight)\n",
    "            \n",
    "        return encoder_output, encoder_attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 后续注意力掩码\n",
    "def get_subsequent_attn_mask(decoder_input):\n",
    "    return torch.triu(torch.ones(decoder_input.size(0), decoder_input.size(1), decoder_input.size(1)), diagonal=1).byte()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解码器层\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, dim_embedding, dim_Q, dim_K, dim_V, n_head, dim_pwffn):\n",
    "        super().__init__()\n",
    "        self.DecoderLayer_MultiHeadAttention = MultiHeadAttention(dim_embedding, dim_Q, dim_K, dim_V, n_head)\n",
    "        self.EncoderDecoderLayer_MultiHeadAttention = MultiHeadAttention(dim_embedding, dim_Q, dim_K, dim_V, n_head)\n",
    "        self.DecoderLayer_PositionWiseFeedForwardNet = PositionWiseFeedForwardNet(dim_embedding, dim_pwffn)\n",
    "\n",
    "    def forward(self, decoder_layer_input, padding_subsequent_attn_mask, encoder_output, encoder_decoder_padding_attn_mask):\n",
    "        decoder_layer_output, decoder_layer_attn_weight = self.DecoderLayer_MultiHeadAttention(decoder_layer_input, decoder_layer_input, decoder_layer_input, padding_subsequent_attn_mask)\n",
    "        encoder_decoder_layer_output, encoder_decoder_layer_attn_weight = self.EncoderDecoderLayer_MultiHeadAttention(decoder_layer_output, encoder_output, encoder_output, encoder_decoder_padding_attn_mask)\n",
    "        decoder_layer_output = self.DecoderLayer_PositionWiseFeedForwardNet(encoder_decoder_layer_output)\n",
    "        return decoder_layer_output, decoder_layer_attn_weight, encoder_decoder_layer_attn_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解码器\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, corpus, dim_embedding, dim_Q, dim_K, dim_V, n_head, dim_pwffn, n_layer):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(corpus.len_target_vocabulary, dim_embedding)\n",
    "        self.position_embedding = nn.Embedding.from_pretrained(get_sin_function_positional_coding_table(corpus.max_len_target_sequence + 1, dim_embedding), freeze=True)\n",
    "        self.layers = nn.ModuleList(DecoderLayer(dim_embedding, dim_Q, dim_K, dim_V, n_head, dim_pwffn) for _ in range(n_layer))\n",
    "        \n",
    "    def forward(self, decoder_input, encoder_input, encoder_output):\n",
    "        positional_coding_template = torch.arange(1, decoder_input.size(1) + 1).unsqueeze(0)\n",
    "        decoder_output = self.token_embedding(decoder_input) + self.position_embedding(positional_coding_template)\n",
    "\n",
    "        decoder_padding_attn__mask = get_padding_attn_mask(decoder_input, decoder_input)\n",
    "        decoder_subsequent_attn_mask = get_subsequent_attn_mask(decoder_input)\n",
    "        decoder_padding_subsequent_attn_mask = torch.gt((decoder_padding_attn__mask + decoder_subsequent_attn_mask), 0)\n",
    "        encoder_decoder_padding_attn_mask = get_padding_attn_mask(decoder_input, encoder_input)\n",
    "\n",
    "        decoder_attn_weights, encoder_decoder_attn_weights = [], []\n",
    "        for layer in self.layers:\n",
    "            decoder_output, decoder_attn_weight, encoder_decoder_attn_weight = layer(decoder_output, decoder_padding_subsequent_attn_mask, encoder_output, encoder_decoder_padding_attn_mask)\n",
    "            decoder_attn_weights.append(decoder_attn_weight)\n",
    "            encoder_decoder_attn_weights.append(encoder_decoder_attn_weight)\n",
    "\n",
    "        return decoder_output, decoder_attn_weights, encoder_decoder_attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, corpus, dim_embedding, dim_Q, dim_K, dim_V, n_head, dim_pwffn, n_layer):\n",
    "        super().__init__()\n",
    "        self.Encoder = Encoder(corpus, dim_embedding, dim_Q, dim_K, dim_V, n_head, dim_pwffn, n_layer)\n",
    "        self.Decoder = Decoder(corpus, dim_embedding, dim_Q, dim_K, dim_V, n_head, dim_pwffn, n_layer)\n",
    "        self.projection = nn.Linear(dim_embedding, corpus.len_target_vocabulary, bias=False)\n",
    "\n",
    "    def forward(self, encoder_input, decoder_input):\n",
    "        encoder_output, encoder_attn_weights = self.Encoder(encoder_input)\n",
    "        decoder_output, decoder_attn_weights, encoder_decoder_attn_weights = self.Decoder(decoder_input, encoder_input, encoder_output)\n",
    "        decoder_logits = self.projection(decoder_output)\n",
    "        return decoder_logits, encoder_attn_weights, decoder_attn_weights, encoder_decoder_attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationCorpus:\n",
    "    def __init__(self, sentences):\n",
    "        self.sentences = sentences\n",
    "        self.max_len_source_sequence = max(len(sentence[0].split()) for sentence in sentences) + 1\n",
    "        self.max_len_target_sequence = max(len(sentence[1].split()) for sentence in sentences) + 2\n",
    "        self.source_vocabulary, self.target_vocabulary, self.len_source_vocabulary, self.len_target_vocabulary = self.create_vocabulary()\n",
    "        self.index_vocabulary_to_source_vocabulary = {v:k for k, v in self.source_vocabulary.items()}\n",
    "        self.index_vocabulary_to_target_vocabulary = {v:k for k, v in self.target_vocabulary.items()}\n",
    "\n",
    "    def create_vocabulary(self):\n",
    "        source_counter = Counter(token for sentence in self.sentences for token in sentence[0].split())\n",
    "        target_counter = Counter(token for sentence in self.sentences for token in sentence[1].split())\n",
    "        source_vocabulary = {'<pad>': 0, **{token: i + 1 for i, token in enumerate(source_counter)}}\n",
    "        target_vocabulary = {'<pad>': 0, '<sos>': 1, '<eos>': 2, **{token: i + 3 for i, token in enumerate(target_counter)}}\n",
    "        len_source_vocabulary = len(source_vocabulary)\n",
    "        len_target_vocabulary = len(target_vocabulary)\n",
    "        return source_vocabulary, target_vocabulary, len_source_vocabulary, len_target_vocabulary\n",
    "    \n",
    "    def batch_dataset(self, batch_size, if_train_dataset=True):\n",
    "        feature_dataset, label_dataset, target_dataset = [], [], []\n",
    "        sentence_indices = torch.randperm(len(self.sentences))[:batch_size]\n",
    "        for index in sentence_indices:\n",
    "            source_sentence, target_sentence = self.sentences[index]\n",
    "            source_sentence_to_index_sentence = [self.source_vocabulary[token] for token in source_sentence.split()]\n",
    "            target_sentence_to_index_sentence = [self.target_vocabulary['<sos>']] + [self.target_vocabulary[token] for token in target_sentence.split()] + [self.target_vocabulary['<eos>']]\n",
    "            source_sentence_to_index_sentence += [self.source_vocabulary['<pad>']] * (self.max_len_source_sequence - len(source_sentence_to_index_sentence))\n",
    "            target_sentence_to_index_sentence += [self.target_vocabulary['<pad>']] * (self.max_len_target_sequence - len(target_sentence_to_index_sentence))\n",
    "            feature_dataset.append(source_sentence_to_index_sentence)\n",
    "            label_dataset.append([self.target_vocabulary['<sos>']] + ([self.target_vocabulary['<pad>']] * (self.max_len_target_sequence - 2)) if not if_train_dataset else target_sentence_to_index_sentence[:-1])\n",
    "            target_dataset.append(target_sentence_to_index_sentence[1:])\n",
    "        return torch.LongTensor(feature_dataset), torch.LongTensor(label_dataset), torch.LongTensor(target_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    ['咖哥 喜欢 小冰', 'KaGe likes XiaoBing'],\n",
    "    ['我 爱 学习 人工智能', 'I love studying AI'],\n",
    "    ['深度学习 改变 世界', 'DL changed the world'],\n",
    "    ['自然语言处理 很 强大', 'NLP is powerful'],\n",
    "    ['神将网络 非常 复杂', 'Neural-networks are complex']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = TranslationCorpus(sentences)\n",
    "dim_embedding = 512\n",
    "dim_Q = dim_K = dim_V = 64\n",
    "n_head = 8\n",
    "dim_pwffn = 2048\n",
    "n_layer = 6\n",
    "batch_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(corpus, dim_embedding, dim_Q, dim_K, dim_V, n_head, dim_pwffn, n_layer)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1 cost=2.837945\n",
      "Epoch:2 cost=3.554357\n",
      "Epoch:3 cost=1.618354\n",
      "Epoch:4 cost=1.308300\n",
      "Epoch:5 cost=1.379172\n",
      "Epoch:6 cost=0.976445\n",
      "Epoch:7 cost=1.056664\n",
      "Epoch:8 cost=0.629448\n",
      "Epoch:9 cost=0.303300\n",
      "Epoch:10 cost=0.292353\n",
      "Epoch:11 cost=0.201005\n",
      "Epoch:12 cost=0.160397\n",
      "Epoch:13 cost=0.074571\n",
      "Epoch:14 cost=0.050659\n",
      "Epoch:15 cost=0.057507\n",
      "Epoch:16 cost=0.037399\n",
      "Epoch:17 cost=0.032547\n",
      "Epoch:18 cost=0.044597\n",
      "Epoch:19 cost=0.037180\n",
      "Epoch:20 cost=0.025808\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    train_feature_dataset, train_label_dataset, train_target_dataset = corpus.batch_dataset(batch_size, True)\n",
    "    train_decoder_logits, _, _, _ = model(train_feature_dataset, train_label_dataset)\n",
    "    loss = criterion(train_decoder_logits.view(-1, corpus.len_target_vocabulary), train_target_dataset.view(-1))\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        print(f'Epoch:{epoch + 1} cost={loss:.6f}')\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "咖哥喜欢小冰<pad><pad>   ----->   ['KaGe', 'likes', '<eos>', '<eos>', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "eval_feature_dataset, eval_label_dataset, eval_target_dataset = corpus.batch_dataset(1, False)\n",
    "eval_decoder_logits, eval_encoder_attn_weights, eval_decoder_attn_weights, eval_encoder_decoder_attn_weights = model(eval_feature_dataset, eval_label_dataset)\n",
    "eval_decoder_logits = eval_decoder_logits.view(-1, corpus.len_target_vocabulary).max(1, keepdim=True)[1]\n",
    "translate_sentence = [corpus.index_vocabulary_to_target_vocabulary[index.item()] for index in eval_decoder_logits.squeeze()]\n",
    "input_sentence = ''.join(corpus.index_vocabulary_to_source_vocabulary[index.item()] for index in eval_feature_dataset[0])\n",
    "print(input_sentence, '  ----->  ', translate_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 贪婪解码器\n",
    "def greedy_decoder(model, encoder_input, start_symbol):\n",
    "    encoder_output, _ = model.Encoder(encoder_input)\n",
    "    decoder_input = torch.zeros(1, 5).type_as(encoder_input)\n",
    "    next_symbol = start_symbol\n",
    "    for i in range(0, 5):\n",
    "        decoder_input[0][i] = next_symbol\n",
    "        decoder_output, _, _ = model.Decoder(decoder_input, encoder_input, encoder_output)\n",
    "        project = model.projection(decoder_output)\n",
    "        project = project.squeeze(0).max(dim=-1, keepdim=False)[1]\n",
    "        next_token = project[i]\n",
    "        next_symbol = next_token.item()\n",
    "    decoder_output = decoder_input\n",
    "    return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "咖哥喜欢小冰<pad><pad>   ----->   ['<sos>', 'KaGe', 'likes', 'XiaoBing', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "greedy_decoder_output = greedy_decoder(model, eval_feature_dataset, corpus.target_vocabulary['<sos>'])\n",
    "greedy_translate_sentence = [corpus.index_vocabulary_to_target_vocabulary[index.item()] for index in greedy_decoder_output.squeeze()]\n",
    "greedy_input_sentence = ''.join(corpus.index_vocabulary_to_source_vocabulary[index.item()] for index in eval_feature_dataset[0])\n",
    "print(greedy_input_sentence, '  ----->  ', greedy_translate_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
