{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from collections import Counter\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正弦函数位置编码表\n",
    "def get_sin_function_positional_coding_table(max_len_sequence, dim_embedding):\n",
    "    sin_function_positional_coding_table = torch.zeros(max_len_sequence, dim_embedding)\n",
    "    for pos_i in range(max_len_sequence):\n",
    "        for hid_j in range(dim_embedding):\n",
    "            sin_function_positional_coding_table[pos_i, hid_j] = pos_i / torch.pow(torch.tensor(10000.0), 2 * (hid_j // 2) / dim_embedding)\n",
    "    sin_function_positional_coding_table[:, 0::2] = torch.sin(sin_function_positional_coding_table[:, 0::2])\n",
    "    sin_function_positional_coding_table[:, 1::2] = torch.sin(sin_function_positional_coding_table[:, 1::2])\n",
    "    return sin_function_positional_coding_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 填充注意力掩码\n",
    "def get_padding_attn_mask(sequence_Q, sequence_K):\n",
    "    batch_size, len_Q = sequence_Q.size()\n",
    "    batch_size, len_K = sequence_K.size()\n",
    "    padding_attn_mask = sequence_K.eq(0).unsqueeze(1).expand(batch_size, len_Q, len_K)\n",
    "    return padding_attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 缩放-点积-填充掩码-注意力\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, padding_attn_mask):\n",
    "        scale_weight = torch.matmul(Q, K.transpose(-2, -1)) / torch.pow(torch.tensor(Q.size(-1)), 2)\n",
    "        scale_weight.masked_fill(padding_attn_mask, -1e9)\n",
    "        attn_weight = F.softmax(scale_weight, dim=-1)\n",
    "        context = torch.matmul(attn_weight, V)\n",
    "        return context, attn_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多头-缩放-点积-填充掩码-注意力\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_embedding, dim_Q, dim_K, dim_V, n_head):\n",
    "        super().__init__()\n",
    "        self.dim_Q = dim_Q\n",
    "        self.dim_K = dim_K\n",
    "        self.dim_V = dim_V\n",
    "        self.n_head = n_head\n",
    "\n",
    "        self.multi_head_linear_Q = nn.Linear(dim_embedding, self.dim_Q * n_head)\n",
    "        self.multi_head_linear_K = nn.Linear(dim_embedding, self.dim_K * n_head)\n",
    "        self.multi_head_linear_V = nn.Linear(dim_embedding, self.dim_V * n_head)\n",
    "\n",
    "        self.MultiHeadAttention_ScaledDotProductAttention = ScaledDotProductAttention()\n",
    "        self.linear = nn.Linear(self.n_head * self.dim_V, dim_embedding)\n",
    "        self.layer_norm = nn.LayerNorm(dim_embedding)\n",
    "\n",
    "    def forward(self, Q, K, V, padding_attn_mask):\n",
    "        residual, batch_size = Q, Q.size(0)\n",
    "\n",
    "        multi_head_Q  = self.multi_head_linear_Q(Q).view(batch_size, -1, self.n_head, self.dim_Q).transpose(-3, -2)\n",
    "        multi_head_K  = self.multi_head_linear_K(K).view(batch_size, -1, self.n_head, self.dim_K).transpose(-3, -2)\n",
    "        multi_head_V  = self.multi_head_linear_V(V).view(batch_size, -1, self.n_head, self.dim_V).transpose(-3, -2)\n",
    "\n",
    "        multi_head_padding_attn_mask = padding_attn_mask.unsqueeze(1).repeat(1, self.n_head, 1, 1)\n",
    "\n",
    "        context, attn_weight = self.MultiHeadAttention_ScaledDotProductAttention(multi_head_Q, multi_head_K, multi_head_V, multi_head_padding_attn_mask)\n",
    "        context = context.transpose(-3, -2).contiguous().view(batch_size, -1, self.n_head * self.dim_V)\n",
    "        context = self.layer_norm(self.linear(context) + residual)\n",
    "        return context, attn_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 逐位-卷积-前馈网络\n",
    "class PositionWiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self, dim_embedding, dim_pwffn):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(dim_embedding, dim_pwffn, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(dim_pwffn, dim_embedding, kernel_size=1)\n",
    "        self.layer_norm = nn.LayerNorm(dim_embedding)\n",
    "\n",
    "    def forward(self, context):\n",
    "        residual = context\n",
    "        context = F.relu(self.conv1(context.transpose(1, 2)))\n",
    "        context = self.conv2(context).transpose(1, 2)\n",
    "        context = self.layer_norm(context + residual)\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编码器层\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, dim_embedding, dim_Q, dim_K, dim_V, n_head, dim_pwffn):\n",
    "        super().__init__()\n",
    "        self.EncoderLayer_MultiHeadAttention = MultiHeadAttention(dim_embedding, dim_Q, dim_K, dim_V, n_head)\n",
    "        self.EncoderLayer_PositionWiseFeedForwardNet = PositionWiseFeedForwardNet(dim_embedding, dim_pwffn)\n",
    "\n",
    "    def forward(self, encoder_layer_input, encoder_layer_padding_attn_mask):\n",
    "        encoder_layer_output, encoder_layer_output_attn_weight = self.EncoderLayer_MultiHeadAttention(encoder_layer_input, encoder_layer_input, encoder_layer_input, encoder_layer_padding_attn_mask)\n",
    "        encoder_layer_output = self.EncoderLayer_PositionWiseFeedForwardNet(encoder_layer_output)\n",
    "        return encoder_layer_output, encoder_layer_output_attn_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编码器\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, corpus, dim_embedding, dim_Q, dim_K, dim_V, n_head, dim_pwffn, n_layer):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(corpus.len_source_vocabulary, dim_embedding)\n",
    "        self.position_embedding = nn.Embedding.from_pretrained(get_sin_function_positional_coding_table(corpus.max_len_source_sequence + 1, dim_embedding), freeze=True)\n",
    "        self.layers = nn.ModuleList(EncoderLayer(dim_embedding, dim_Q, dim_K, dim_V, n_head, dim_pwffn) for _ in range(n_layer))\n",
    "\n",
    "    def forward(self, encoder_input):\n",
    "        positional_coding_template = torch.arange(1, encoder_input.size(1) + 1).unsqueeze(0)\n",
    "        encoder_output = self.token_embedding(encoder_input) + self.position_embedding(positional_coding_template)\n",
    "\n",
    "        encoder_padding_attn_mask = get_padding_attn_mask(encoder_input, encoder_input)\n",
    "\n",
    "        encoder_attn_weights = []\n",
    "        for layer in self.layers:\n",
    "            encoder_output, encoder_attn_weight = layer(encoder_output, encoder_padding_attn_mask)\n",
    "            encoder_attn_weights.append(encoder_attn_weight)\n",
    "            \n",
    "        return encoder_output, encoder_attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 后续注意力掩码\n",
    "def get_subsequent_attn_mask(decoder_input):\n",
    "    return torch.triu(torch.ones(decoder_input.size(0), decoder_input.size(1), decoder_input.size(1)), diagonal=1).byte()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解码器层\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, dim_embedding, dim_Q, dim_K, dim_V, n_head, dim_pwffn):\n",
    "        super().__init__()\n",
    "        self.DecoderLayer_MultiHeadAttention = MultiHeadAttention(dim_embedding, dim_Q, dim_K, dim_V, n_head)\n",
    "        self.EncoderDecoderLayer_MultiHeadAttention = MultiHeadAttention(dim_embedding, dim_Q, dim_K, dim_V, n_head)\n",
    "        self.DecoderLayer_PositionWiseFeedForwardNet = PositionWiseFeedForwardNet(dim_embedding, dim_pwffn)\n",
    "\n",
    "    def forward(self, decoder_layer_input, padding_subsequent_attn_mask, encoder_output, encoder_decoder_padding_attn_mask):\n",
    "        decoder_layer_output, decoder_layer_attn_weight = self.DecoderLayer_MultiHeadAttention(decoder_layer_input, decoder_layer_input, decoder_layer_input, padding_subsequent_attn_mask)\n",
    "        encoder_decoder_layer_output, encoder_decoder_layer_attn_weight = self.EncoderDecoderLayer_MultiHeadAttention(decoder_layer_output, encoder_output, encoder_output, encoder_decoder_padding_attn_mask)\n",
    "        decoder_layer_output = self.DecoderLayer_PositionWiseFeedForwardNet(encoder_decoder_layer_output)\n",
    "        return decoder_layer_output, decoder_layer_attn_weight, encoder_decoder_layer_attn_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解码器\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, corpus, dim_embedding, dim_Q, dim_K, dim_V, n_head, dim_pwffn, n_layer):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(corpus.len_target_vocabulary, dim_embedding)\n",
    "        self.position_embedding = nn.Embedding.from_pretrained(get_sin_function_positional_coding_table(corpus.max_len_target_sequence + 1, dim_embedding), freeze=True)\n",
    "        self.layers = nn.ModuleList(DecoderLayer(dim_embedding, dim_Q, dim_K, dim_V, n_head, dim_pwffn) for _ in range(n_layer))\n",
    "        \n",
    "    def forward(self, decoder_input, encoder_input, encoder_output):\n",
    "        positional_coding_template = torch.arange(1, decoder_input.size(1) + 1).unsqueeze(0)\n",
    "        decoder_output = self.token_embedding(decoder_input) + self.position_embedding(positional_coding_template)\n",
    "\n",
    "        decoder_padding_attn__mask = get_padding_attn_mask(decoder_input, decoder_input)\n",
    "        decoder_subsequent_attn_mask = get_subsequent_attn_mask(decoder_input)\n",
    "        decoder_padding_subsequent_attn_mask = torch.gt((decoder_padding_attn__mask + decoder_subsequent_attn_mask), 0)\n",
    "        encoder_decoder_padding_attn_mask = get_padding_attn_mask(decoder_input, encoder_input)\n",
    "\n",
    "        decoder_attn_weights, encoder_decoder_attn_weights = [], []\n",
    "        for layer in self.layers:\n",
    "            decoder_output, decoder_attn_weight, encoder_decoder_attn_weight = layer(decoder_output, decoder_padding_subsequent_attn_mask, encoder_output, encoder_decoder_padding_attn_mask)\n",
    "            decoder_attn_weights.append(decoder_attn_weight)\n",
    "            encoder_decoder_attn_weights.append(encoder_decoder_attn_weight)\n",
    "\n",
    "        return decoder_output, decoder_attn_weights, encoder_decoder_attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, corpus, dim_embedding, dim_Q, dim_K, dim_V, n_head, dim_pwffn, n_layer):\n",
    "        super().__init__()\n",
    "        self.Encoder = Encoder(corpus, dim_embedding, dim_Q, dim_K, dim_V, n_head, dim_pwffn, n_layer)\n",
    "        self.Decoder = Decoder(corpus, dim_embedding, dim_Q, dim_K, dim_V, n_head, dim_pwffn, n_layer)\n",
    "        self.projection = nn.Linear(dim_embedding, corpus.len_target_vocabulary, bias=False)\n",
    "\n",
    "    def forward(self, encoder_input, decoder_input):\n",
    "        encoder_output, encoder_attn_weights = self.Encoder(encoder_input)\n",
    "        decoder_output, decoder_attn_weights, encoder_decoder_attn_weights = self.Decoder(decoder_input, encoder_input, encoder_output)\n",
    "        decoder_logits = self.projection(decoder_output)\n",
    "        return decoder_logits, encoder_attn_weights, decoder_attn_weights, encoder_decoder_attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationCorpus:\n",
    "    def __init__(self, sentences):\n",
    "        self.sentences = sentences\n",
    "        self.max_len_source_sequence = max(len(sentence[0].split()) for sentence in sentences) + 1\n",
    "        self.max_len_target_sequence = max(len(sentence[1].split()) for sentence in sentences) + 2\n",
    "        self.source_vocabulary, self.target_vocabulary, self.len_source_vocabulary, self.len_target_vocabulary = self.create_vocabulary()\n",
    "        self.index_vocabulary_to_source_vocabulary = {v:k for k, v in self.source_vocabulary.items()}\n",
    "        self.index_vocabulary_to_target_vocabulary = {v:k for k, v in self.target_vocabulary.items()}\n",
    "\n",
    "    def create_vocabulary(self):\n",
    "        source_counter = Counter(token for sentence in self.sentences for token in sentence[0].split())\n",
    "        target_counter = Counter(token for sentence in self.sentences for token in sentence[1].split())\n",
    "        source_vocabulary = {'<pad>': 0, **{token: i + 1 for i, token in enumerate(source_counter)}}\n",
    "        target_vocabulary = {'<pad>': 0, '<sos>': 1, '<eos>': 2, **{token: i + 3 for i, token in enumerate(target_counter)}}\n",
    "        len_source_vocabulary = len(source_vocabulary)\n",
    "        len_target_vocabulary = len(target_vocabulary)\n",
    "        return source_vocabulary, target_vocabulary, len_source_vocabulary, len_target_vocabulary\n",
    "    \n",
    "    def batch_dataset(self, batch_size, if_train_dataset=True):\n",
    "        feature_dataset, label_dataset, target_dataset = [], [], []\n",
    "        sentence_indices = torch.randperm(len(self.sentences))[:batch_size]\n",
    "        for index in sentence_indices:\n",
    "            source_sentence, target_sentence = self.sentences[index]\n",
    "            source_sentence_to_index_sentence = [self.source_vocabulary[token] for token in source_sentence.split()]\n",
    "            target_sentence_to_index_sentence = [self.target_vocabulary['<sos>']] + [self.target_vocabulary[token] for token in target_sentence.split()] + [self.target_vocabulary['<eos>']]\n",
    "            source_sentence_to_index_sentence += [self.source_vocabulary['<pad>']] * (self.max_len_source_sequence - len(source_sentence_to_index_sentence))\n",
    "            target_sentence_to_index_sentence += [self.target_vocabulary['<pad>']] * (self.max_len_target_sequence - len(target_sentence_to_index_sentence))\n",
    "            feature_dataset.append(source_sentence_to_index_sentence)\n",
    "            label_dataset.append([self.target_vocabulary['<sos>']] + ([self.target_vocabulary['<pad>']] * (self.max_len_target_sequence - 2)) if if_train_dataset else target_sentence_to_index_sentence[:-1])\n",
    "            target_dataset.append(target_sentence_to_index_sentence[1:])\n",
    "        return torch.LongTensor(feature_dataset), torch.LongTensor(label_dataset), torch.LongTensor(target_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    ['咖哥 喜欢 小冰', 'KaGe likes XiaoBing'],\n",
    "    ['我 爱 学习 人工智能', 'I love studying AI'],\n",
    "    ['深度学习 改变 世界', 'DL changed the world'],\n",
    "    ['自然语言处理 很 强大', 'NLP is powerful'],\n",
    "    ['神将网络 非常 复杂', 'Neural-networks are complex']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = TranslationCorpus(sentences)\n",
    "dim_embedding = 512\n",
    "dim_Q = dim_K = dim_V = 64\n",
    "n_head = 8\n",
    "dim_pwffn = 2048\n",
    "n_layer = 6\n",
    "batch_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(corpus, dim_embedding, dim_Q, dim_K, dim_V, n_head, dim_pwffn, n_layer)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1 cost=3.378279\n",
      "Epoch:2 cost=3.832541\n",
      "Epoch:3 cost=2.703499\n",
      "Epoch:4 cost=2.520695\n",
      "Epoch:5 cost=2.721653\n",
      "Epoch:6 cost=2.365467\n",
      "Epoch:7 cost=2.063673\n",
      "Epoch:8 cost=2.157883\n",
      "Epoch:9 cost=1.965703\n",
      "Epoch:10 cost=1.666100\n",
      "Epoch:11 cost=1.383447\n",
      "Epoch:12 cost=1.764755\n",
      "Epoch:13 cost=1.557842\n",
      "Epoch:14 cost=1.421220\n",
      "Epoch:15 cost=1.245278\n",
      "Epoch:16 cost=1.203177\n",
      "Epoch:17 cost=1.257416\n",
      "Epoch:18 cost=1.239156\n",
      "Epoch:19 cost=1.131092\n",
      "Epoch:20 cost=1.143828\n",
      "Epoch:21 cost=1.085775\n",
      "Epoch:22 cost=1.161703\n",
      "Epoch:23 cost=1.035513\n",
      "Epoch:24 cost=0.945574\n",
      "Epoch:25 cost=0.963884\n",
      "Epoch:26 cost=0.877381\n",
      "Epoch:27 cost=0.896587\n",
      "Epoch:28 cost=0.771263\n",
      "Epoch:29 cost=0.758602\n",
      "Epoch:30 cost=0.664176\n",
      "Epoch:31 cost=0.653280\n",
      "Epoch:32 cost=0.613909\n",
      "Epoch:33 cost=0.655209\n",
      "Epoch:34 cost=0.492556\n",
      "Epoch:35 cost=0.405615\n",
      "Epoch:36 cost=0.437012\n",
      "Epoch:37 cost=0.459177\n",
      "Epoch:38 cost=0.386451\n",
      "Epoch:39 cost=0.455268\n",
      "Epoch:40 cost=0.471318\n",
      "Epoch:41 cost=0.391468\n",
      "Epoch:42 cost=0.432507\n",
      "Epoch:43 cost=0.344285\n",
      "Epoch:44 cost=0.356254\n",
      "Epoch:45 cost=0.369848\n",
      "Epoch:46 cost=0.365710\n",
      "Epoch:47 cost=0.232270\n",
      "Epoch:48 cost=0.260481\n",
      "Epoch:49 cost=0.188484\n",
      "Epoch:50 cost=0.315341\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    train_feature_dataset, train_label_dataset, train_target_dataset = corpus.batch_dataset(batch_size, True)\n",
    "    train_decoder_logits, _, _, _ = model(train_feature_dataset, train_label_dataset)\n",
    "    loss = criterion(train_decoder_logits.view(-1, corpus.len_target_vocabulary), train_target_dataset.view(-1))\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        print(f'Epoch:{epoch + 1} cost={loss:.6f}')\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我爱学习人工智能<pad>   ----->   ['I', 'I', 'I', 'AI', 'AI']\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "eval_feature_dataset, eval_label_dataset, eval_target_dataset = corpus.batch_dataset(1, False)\n",
    "eval_decoder_logits, eval_encoder_attn_weights, eval_decoder_attn_weights, eval_encoder_decoder_attn_weights = model(eval_feature_dataset, eval_label_dataset)\n",
    "eval_decoder_logits = eval_decoder_logits.view(-1, corpus.len_target_vocabulary).max(1, keepdim=True)[1]\n",
    "translate_sentence = [corpus.index_vocabulary_to_target_vocabulary[index.item()] for index in eval_decoder_logits.squeeze()]\n",
    "input_sentence = ''.join(corpus.index_vocabulary_to_source_vocabulary[index.item()] for index in eval_feature_dataset[0])\n",
    "print(input_sentence, '  ----->  ', translate_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
